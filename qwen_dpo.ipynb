{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de778031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "def create_qwen_model():\n",
    "    model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-3B-Instruct', \n",
    "                                                torch_dtype='auto',\n",
    "                                                device_map='auto')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO training model\n",
    "model_PI, tokenizer = create_qwen_model()\n",
    "# Base model\n",
    "model_base, _ = create_qwen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a224fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, prompt: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, \n",
    "                                         tokenize=False,\n",
    "                                         add_generation_prompt=True,)\n",
    "    # print(text)\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(device)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ca8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple training data\n",
    "train_data = [\n",
    "    {'prompt': '你是谁？', 'favour': '我是通义千问', 'rejected': '我是阿里云开发的大语言模型，我叫通义千问'},\n",
    "    {'prompt': '谁发明了你？', 'favour': '阿里巴巴', 'rejected': 'Ian Goodfellow'},\n",
    "]\n",
    "\n",
    "def dpo_to_message(dpo_pairs):\n",
    "    favour_msg, reject_msg = [], []\n",
    "    for pair in dpo_pairs:\n",
    "        favour_msg.append([{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                           {'role': 'user', 'content': pair['prompt']},\n",
    "                           {'role': 'assistant', 'content': pair['favour']}])\n",
    "        reject_msg.append([{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                           {'role': 'user', 'content': pair['prompt']},\n",
    "                           {'role': 'assistant', 'content': pair['rejected']}])\n",
    "    return favour_msg, reject_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3a904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "def preprocess(tokenizer, batch_messages):\n",
    "    input_ls, target_ls = list(), list()\n",
    "\n",
    "    im_start_id = tokenizer('<|im_start|>').input_ids\n",
    "    im_end_id = tokenizer('<|im_end|>').input_ids\n",
    "    newline_id = tokenizer('\\n').input_ids\n",
    "    padding_id = tokenizer('<|endoftext|>').input_ids\n",
    "    ignore = [-100]\n",
    "\n",
    "    for messages in batch_messages:\n",
    "        input_ids, target_ids = list(), list()\n",
    "        for msg in messages:\n",
    "            role_id = tokenizer(msg['role']).input_ids\n",
    "            content_id = tokenizer(msg['content']).input_ids\n",
    "            if msg['role'] in ['system', 'user']:\n",
    "                ignore_parts = role_id + content_id + newline_id\n",
    "                input_ids.extend(im_start_id + ignore_parts + im_end_id + newline_id)\n",
    "                target_ids.extend(im_start_id + ignore * len(ignore_parts) + im_end_id + newline_id)\n",
    "            else:\n",
    "                ignore_parts = role_id + newline_id\n",
    "                input_ids.extend(im_start_id + ignore_parts + content_id + im_end_id + newline_id)\n",
    "                target_ids.extend(im_start_id + ignore * len(ignore_parts) + content_id + im_end_id + newline_id)\n",
    "        input_ls.append(input_ids)\n",
    "        target_ls.append(target_ids)\n",
    "    \n",
    "    # padding\n",
    "    max_len = max(len(input_ids) for input_ids in input_ls)\n",
    "    for input_ids, target_ids in zip(input_ls, target_ls):\n",
    "        input_ids.extend(padding_id * (max_len - len(input_ids)))\n",
    "        target_ids.extend(ignore * (max_len - len(target_ids)))\n",
    "    batch_input_idx = torch.tensor(input_ls, dtype=torch.long)\n",
    "    batch_target_idx = torch.tensor(target_ls, dtype=torch.long)\n",
    "    batch_mask = batch_input_idx.ne(padding_id[0]).type(torch.long)  # padding mask\n",
    "    return batch_input_idx, batch_target_idx, batch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01524015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training mode\n",
    "model_PI.train()\n",
    "model_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe8ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_prob_calc(target_ids, pi_logits, base_logits):\n",
    "    pi_prob = torch.log_softmax(pi_logits, dim=-1)\n",
    "    base_prob = torch.log_softmax(base_logits, dim=-1)\n",
    "    \n",
    "    # torch.gather()\n",
    "    ignore_mask = target_ids != -100\n",
    "    indexes = target_ids * ignore_mask\n",
    "\n",
    "    # Gather the probabilities for the target tokens, ignoring the -100 tokens\n",
    "    pi_prob_wrt_target = torch.gather(pi_prob, dim=-1, index=indexes.unsqueeze(-1)).squeeze(-1) * ignore_mask\n",
    "    base_prob_wrt_target = torch.gather(base_prob, dim=-1, index=indexes.unsqueeze(-1)).squeeze(-1) * ignore_mask\n",
    "\n",
    "    # Calculate the final probabilities by averaging over the valid tokens\n",
    "    pi_prob_final = pi_prob_wrt_target.sum(-1) / ignore_mask.sum(-1)\n",
    "    base_prob_final = base_prob_wrt_target.sum(-1) / ignore_mask.sum(-1)\n",
    "    return pi_prob_final, base_prob_final\n",
    "\n",
    "def dpo_loss_calc(params):\n",
    "    chosen_target_ids = params['chosen_target_ids'][:, 1:]\n",
    "    pi_chosen_logits = params['pi_chosen_logits'][:, :-1, :]\n",
    "    base_chosen_logits = params['base_chosen_logits'][:, :-1, :]\n",
    "    pi_chosen_prob, base_chosen_prob = dpo_prob_calc(chosen_target_ids, pi_chosen_logits, base_chosen_logits)\n",
    "\n",
    "    reject_target_ids = params['reject_target_ids'][:, 1:]\n",
    "    pi_reject_logits = params['pi_reject_logits'][:, :-1, :]\n",
    "    base_reject_logits = params['base_reject_logits'][:, :-1, :]\n",
    "    pi_reject_prob, base_reject_prob = dpo_prob_calc(reject_target_ids, pi_reject_logits, base_reject_logits)\n",
    "\n",
    "    # Calculate the DPO loss\n",
    "    pi_prob_diff = pi_chosen_prob - pi_reject_prob\n",
    "    base_prob_diff = base_chosen_prob - base_reject_prob\n",
    "    beta = .1\n",
    "    loss = torch.nn.functional.logsigmoid(beta * (pi_prob_diff - base_prob_diff)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94845750",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_PI.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 10\n",
    "vocab = tokenizer.get_vocab()\n",
    "for epoch in range(epochs):\n",
    "    chosen_msg, reject_msg = dpo_to_message(train_data)\n",
    "    # model input and target\n",
    "    chosen_input_ids, chosen_target_ids, chosen_mask_ids = preprocess(tokenizer, chosen_msg)\n",
    "    reject_input_ids, reject_target_ids, reject_mask_ids = preprocess(tokenizer, reject_msg)\n",
    "    # predict of model_PI\n",
    "    pi_chosen_logits = model_PI(input_ids=chosen_input_ids.to(device), attention_mask=chosen_mask_ids.to(device)).logits\n",
    "    pi_reject_logits = model_PI(input_ids=reject_input_ids.to(device), attention_mask=reject_mask_ids.to(device)).logits\n",
    "    # predict of model_base\n",
    "    base_chosen_logits = model_base(chosen_input_ids.to(device), attention_mask=chosen_mask_ids.to(device)).logits\n",
    "    base_reject_logits = model_base(reject_input_ids.to(device), attention_mask=reject_mask_ids.to(device)).logits\n",
    "    # calculate loss\n",
    "    loss = dpo_loss_calc({\n",
    "        'chosen_target_ids': chosen_target_ids.to(device),\n",
    "        'reject_target_ids': reject_target_ids.to(device),\n",
    "        'pi_chosen_logits': pi_chosen_logits.to(device),\n",
    "        'pi_reject_logits': pi_reject_logits.to(device),\n",
    "        'base_chosen_logits': base_chosen_logits.to(device),\n",
    "        'base_reject_logits': base_reject_logits.to(device),\n",
    "    })\n",
    "    print(f'Epoch: {epoch + 1}/{epochs}, Loss: {loss:.3f}')\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PI.eval()\n",
    "# Test the trained model\n",
    "prompts = ['你是谁？', '谁发明了你？', '讲讲Transformer模型']\n",
    "for prompt in prompts:\n",
    "    response = chat(model_PI, tokenizer, prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
