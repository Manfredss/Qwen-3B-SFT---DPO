{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Qwen/Qwen2.5-3B-Instruct',\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto',)\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5e13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model and tokenizer are loaded correctly\n",
    "# print(model)\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f6a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "def chat(prompt: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, \n",
    "                                         tokenize=False,\n",
    "                                         add_generation_prompt=True,)\n",
    "    # print(text)\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(device)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "response = chat(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047c18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, batch_messages):\n",
    "    input_ls, target_ls = list(), list()\n",
    "\n",
    "    im_start_id = tokenizer('<|im_start|>').input_ids\n",
    "    im_end_id = tokenizer('<|im_end|>').input_ids\n",
    "    newline_id = tokenizer('\\n').input_ids\n",
    "    padding_id = tokenizer('<|endoftext|>').input_ids\n",
    "    ignore = [-100]\n",
    "\n",
    "    for messages in batch_messages:\n",
    "        input_ids, target_ids = list(), list()\n",
    "        for msg in messages:\n",
    "            role_id = tokenizer(msg['role']).input_ids\n",
    "            content_id = tokenizer(msg['content']).input_ids\n",
    "            if msg['role'] in ['system', 'user']:\n",
    "                ignore_parts = role_id + content_id + newline_id\n",
    "                input_ids.extend(im_start_id + ignore_parts + im_end_id + newline_id)\n",
    "                target_ids.extend(im_start_id + ignore * len(ignore_parts) + im_end_id + newline_id)\n",
    "            else:\n",
    "                ignore_parts = role_id + newline_id\n",
    "                input_ids.extend(im_start_id + ignore_parts + content_id + im_end_id + newline_id)\n",
    "                target_ids.extend(im_start_id + ignore * len(ignore_parts) + content_id + im_end_id + newline_id)\n",
    "        input_ls.append(input_ids)\n",
    "        target_ls.append(target_ids)\n",
    "    \n",
    "    # padding\n",
    "    max_len = max(len(input_ids) for input_ids in input_ls)\n",
    "    for input_ids, target_ids in zip(input_ls, target_ls):\n",
    "        input_ids.extend(padding_id * (max_len - len(input_ids)))\n",
    "        target_ids.extend(ignore * (max_len - len(target_ids)))\n",
    "    batch_input_idx = torch.tensor(input_ls, dtype=torch.long)\n",
    "    batch_target_idx = torch.tensor(target_ls, dtype=torch.long)\n",
    "    batch_mask = batch_input_idx.ne(padding_id[0]).type(torch.long)  # padding mask\n",
    "    return batch_input_idx, batch_target_idx, batch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a32a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 1, loss: 4.75\n",
      ">> epoch: 11, loss: 1.0546875\n",
      ">> epoch: 21, loss: 0.12451171875\n",
      ">> epoch: 31, loss: 0.10791015625\n",
      ">> epoch: 41, loss: 0.0927734375\n",
      ">> epoch: 51, loss: 0.0908203125\n",
      ">> epoch: 61, loss: 0.08349609375\n",
      ">> epoch: 71, loss: 0.0830078125\n",
      ">> epoch: 81, loss: 0.08251953125\n",
      ">> epoch: 91, loss: 0.0751953125\n"
     ]
    }
   ],
   "source": [
    "prompt = '3+3等于几'\n",
    "messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": prompt},\n",
    "             {'role': 'assistant', 'content': '3+3等于5'}],\n",
    "            [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "             {'role': 'user', 'content': prompt},\n",
    "             {'role': 'assistant', 'content': '3+3等于5。'}],\n",
    "]\n",
    "\n",
    "'''\n",
    "输入：<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>assistant\\n3+3等于5<|im_end|>\\n\n",
    "输出：<|im_start|>------------------------------------<|im_end|>\\n<|im_start|>-----------3+3等于5<|im_end|>\\n\n",
    "'''\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "model.train()\n",
    "for i in range(100):\n",
    "    batch_input_ids, batch_target_ids, batch_mask = preprocess(tokenizer, messages)\n",
    "    model_outputs = model(input_ids=batch_input_ids.to(device),\n",
    "                        batch_mask=batch_mask.to(device))\n",
    "    output_tokens = model_outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    logits = model_outputs.logits[:, :-1, :]\n",
    "    targets = batch_target_ids[:, 1:].to(device)\n",
    "    # print(f'logits shape: {logits.shape}')\n",
    "    # print(f'targets shape: {targets.shape}')\n",
    "    \n",
    "    # 计算损失\n",
    "    criterion = CrossEntropyLoss()\n",
    "    loss = criterion(logits.reshape(-1, logits.size(2)), targets.reshape(-1))\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(f'>> epoch: {i+1}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a2aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3+3等于5\n",
      "6+6等于12。\n"
     ]
    }
   ],
   "source": [
    "print(chat('3+3等于几？'))\n",
    "print(chat('6+6等于几？'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
